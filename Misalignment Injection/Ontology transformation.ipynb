{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f4eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# Define the directories\n",
    "axiom_relations = [\n",
    "    \"subClassOf\", \"equivalentClass\", \"propertyRestrictions\",\n",
    "    \"disjointWith\", \"subPropertyOf\", \"domain\", \"range\",\n",
    "    \"characteristics\", \"inverseOf\"\n",
    "]\n",
    "directories = ['Axiom_per_entity', 'Generated CQ', 'generated description']\n",
    "\n",
    "# Load the list of files in each directory\n",
    "files_in_directories = {directory: os.listdir(directory) for directory in directories}\n",
    "\n",
    "total_data = {}\n",
    "for directory in files_in_directories[\"Axiom_per_entity\"]:\n",
    "    with open(f\"Axiom_per_entity/{directory}\", \"r\") as file:\n",
    "        total_data[directory.split(\"_\")[0]] = json.load(file)\n",
    "\n",
    "description_data = {}\n",
    "for directory in files_in_directories[\"generated description\"]:\n",
    "    temp = {}\n",
    "    with open(f\"generated description/{directory}\", \"r\") as file:\n",
    "        for line in file:\n",
    "            temp[json.loads(line)[\"class\"]] = json.loads(line)[\"description\"]\n",
    "    description_data[directory.split(\"_\")[0]] = temp\n",
    "\n",
    "CQ_data = {}\n",
    "for directory in files_in_directories[\"Generated CQ\"]:\n",
    "    temp = {}\n",
    "    with open(f\"Generated CQ/{directory}\", \"r\") as file:\n",
    "        for line in file:\n",
    "            axiom = json.loads(line)[\"axiom\"]\n",
    "            for rela in axiom_relations:\n",
    "                if rela in axiom:\n",
    "                    cls = axiom.split(rela)[0].strip()\n",
    "                    break\n",
    "            if cls not in temp: temp[cls] = []\n",
    "            temp[cls].append({\"axiom\": axiom, \"CQ\": json.loads(line)[\"CQ\"]})\n",
    "    CQ_data[directory.split(\"_\")[0]] = temp\n",
    "\n",
    "\n",
    "for ontology in total_data:\n",
    "    for classorproperty in total_data[ontology]:\n",
    "        for cp in total_data[ontology][classorproperty]:\n",
    "            temp = {\"axiom\" : total_data[ontology][classorproperty][cp], \"description\" : description_data[ontology][cp], \"CQ\": CQ_data[ontology][cp]}\n",
    "            total_data[ontology][classorproperty][cp] = temp\n",
    "with open(\"total_data.json\", \"w\") as json_file:\n",
    "    json.dump(total_data, json_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68ec51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type1 ➔ 207 classes, 59 properties\n",
      "type2 ➔ 207 classes, 58 properties\n",
      "type3 ➔ 208 classes, 12 properties\n",
      "type4 ➔ 712 classes, 100 properties\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"total_data.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    total_data = json.load(json_file)\n",
    "\n",
    "# initialize types\n",
    "type1, type2, type3, type4 = {}, {}, {}, {}\n",
    "types = [type1, type2, type3, type4]\n",
    "for t in types:\n",
    "    for ontology in total_data:\n",
    "        t[ontology] = {\"classes\": {}, \"properties\": {}}\n",
    "\n",
    "class_counts = {id(t): 0 for t in types}\n",
    "prop_counts  = {id(t): 0 for t in types}\n",
    "\n",
    "def has_and_or_some_only_in_axiom(ax):\n",
    "    for v in ax.values():\n",
    "        if isinstance(v, list):\n",
    "            for expr in v:\n",
    "                if re.search(r'\\b(and|or|some|only)\\b', expr):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "for ontology, cps in total_data.items():\n",
    "    for section in (\"classes\", \"properties\"):\n",
    "        for cp, value in cps[section].items():\n",
    "            ax = value.get(\"axiom\", {}) if isinstance(value, dict) else {}\n",
    "\n",
    "            # 1) eligible types\n",
    "            if section == \"classes\":\n",
    "                is_single_cq = (\n",
    "                    \"CQ\" in value\n",
    "                    and isinstance(value[\"CQ\"], list)\n",
    "                    and len(value[\"CQ\"]) == 1\n",
    "                )\n",
    "                eligible = [type3, type4] if is_single_cq else types.copy()\n",
    "            else:\n",
    "                is_empty_axiom = (\n",
    "                    not ax.get(\"characteristics\")\n",
    "                    and ax.get(\"domain\") == [\"None\"]\n",
    "                    and ax.get(\"range\") == [\"None\"]\n",
    "                    and not ax.get(\"subPropertyOf\")\n",
    "                    and not ax.get(\"inverseOf\")\n",
    "                )\n",
    "                eligible = [type3, type4] if is_empty_axiom else types.copy()\n",
    "\n",
    "\n",
    "            #  2) axiom including and/or some/only processing \n",
    "            if not has_and_or_some_only_in_axiom(ax):\n",
    "                eligible = [t for t in eligible if t is not type3]\n",
    "\n",
    "            # 3) classification\n",
    "            if section == \"classes\":\n",
    "                target = min(eligible, key=lambda t: class_counts[id(t)])\n",
    "                class_counts[id(target)] += 1\n",
    "            else:\n",
    "                target = min(eligible, key=lambda t: prop_counts[id(t)])\n",
    "                prop_counts[id(target)] += 1\n",
    "\n",
    "            target[ontology][section][cp] = value\n",
    "\n",
    "# results\n",
    "for idx, t in enumerate(types, start=1):\n",
    "    nc = sum(len(t[ont][\"classes\"]) for ont in t)\n",
    "    np = sum(len(t[ont][\"properties\"]) for ont in t)\n",
    "    print(f\"type{idx} ➔ {nc} classes, {np} properties\")\n",
    "\n",
    "# save to json files\n",
    "with open(\"type1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(type1, f, ensure_ascii=False, indent=2)\n",
    "with open(\"type2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(type2, f, ensure_ascii=False, indent=2)\n",
    "with open(\"type3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(type3, f, ensure_ascii=False, indent=2)\n",
    "with open(\"type4.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(type4, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2386ea76",
   "metadata": {},
   "source": [
    "Type 1: Missing axiom, Type 2: Missing definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# List of possible axiom predicates\n",
    "axiom_relations = [\n",
    "    \"subClassOf\", \"equivalentClass\", \"propertyRestrictions\",\n",
    "    \"disjointWith\", \"subPropertyOf\", \"domain\", \"range\",\n",
    "    \"characteristics\", \"inverseOf\"\n",
    "]\n",
    "\n",
    "def process_type1(input_path='type1.json', output_path='processed_type1.json'):\n",
    "    # 1. Load the original data\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 2. Traverse each ontology\n",
    "    for ontology in data.values():\n",
    "        # process both classes and properties\n",
    "        for section in ('classes', 'properties'):\n",
    "            for name, info in ontology.get(section, {}).items():\n",
    "                all_cq = info.get('CQ', [])\n",
    "                filtered = []\n",
    "                for entry in all_cq:\n",
    "                    ax = entry['axiom']\n",
    "                    # if the axiom is empty or contains \"None\" in domain/range, skip it\n",
    "                    if \" domain None\" in ax or \" range None\" in ax:\n",
    "                        continue\n",
    "                    filtered.append(entry)\n",
    "                # if no CQ entries are found, skip this entity\n",
    "                cq_entries = filtered if filtered else all_cq\n",
    "                if not cq_entries:\n",
    "                    continue\n",
    "                \n",
    "                if section == 'classes':\n",
    "                    preferred = [e for e in cq_entries if ' disjointWith ' in e['axiom']]\n",
    "                else:  # section == 'properties'\n",
    "                    preferred = [e for e in cq_entries if ' inverseOf ' in e['axiom']]\n",
    "                \n",
    "                if preferred:\n",
    "                    sampled = random.choice(preferred)\n",
    "                else:\n",
    "                    sampled = random.choice(cq_entries)\n",
    "\n",
    "                # b) Parse the sampled axiom to find predicate and expression\n",
    "                axiom_str = sampled['axiom']\n",
    "                info['removed axiom'] = axiom_str\n",
    "                predicate = None\n",
    "                for rel in axiom_relations:\n",
    "                    if f\" {rel} \" in axiom_str:\n",
    "                        predicate = rel\n",
    "                        break\n",
    "                if predicate is None:\n",
    "                    # couldn't parse, skip removal\n",
    "                    continue\n",
    "\n",
    "                # split into subject and expression\n",
    "                subject, expr = axiom_str.split(f\" {predicate} \", 1)\n",
    "                subject = subject.strip()\n",
    "                expr = expr.strip()\n",
    "\n",
    "                # c) Remove that expression from the matching list in info['axiom'],\n",
    "                #    but skip deleting \"None\" from domain/range\n",
    "                ax = info.get('axiom', {})\n",
    "                if predicate in ax and isinstance(ax[predicate], list):\n",
    "                    # if it's domain/range and expr is the literal \"None\", skip removal\n",
    "                    if predicate in ('domain', 'range') and expr == \"None\":\n",
    "                        # do not remove the 'None' placeholder\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            ax[predicate].remove(expr)\n",
    "                        except ValueError:\n",
    "                            print(f\"Expression '{expr}' not found in axiom list for {name}.\")\n",
    "                            # ignore if not present\n",
    "\n",
    "                # d) Build Target CQ and Valid CQ without deleting the original 'CQ'\n",
    "                target_cq = sampled['CQ']\n",
    "                valid_cq = [\n",
    "                    question\n",
    "                    for entry in all_cq\n",
    "                    if entry is not sampled\n",
    "                    for question in entry['CQ']\n",
    "                ]\n",
    "\n",
    "                # e) Attach new fields\n",
    "                info['Target CQ'] = target_cq\n",
    "                info['Valid CQ'] = valid_cq\n",
    "\n",
    "    # 3. Write out the processed file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# run for both type1 and type2\n",
    "process_type1(input_path='type classification/type1.json', output_path='Final_type1.json')\n",
    "process_type1(input_path='type classification/type2.json', output_path='processed_type2.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd58a4",
   "metadata": {},
   "source": [
    "Type 3: Misusing axiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2625e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eats some (plants or animals)] and [drinks some water or milk]\n",
      "[eats only (plants or animals)] and [drinks only water or milk]\n",
      "[eats some (plants and animals)] and [drinks some water and milk]\n",
      "[eats only (plants or animals)] and [drinks some water or milk]\n",
      "[eats some (plants or animals)] and [drinks only water and milk]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def _mutate_axiom(expr: str,\n",
    "                  swap_some_only=True,\n",
    "                  swap_and_or=True) -> str:\n",
    "    \n",
    "    stack = ['']\n",
    "    brackets = []\n",
    "    for ch in expr:\n",
    "        if ch in '([{':\n",
    "            stack.append('')\n",
    "            brackets.append(ch)\n",
    "        elif ch in ')]}':\n",
    "            inner = stack.pop()\n",
    "            open_br = brackets.pop()\n",
    "            mutated_inner = _mutate_axiom(inner, swap_some_only, swap_and_or)\n",
    "            close_br = {'(': ')', '[': ']', '{': '}'}[open_br]\n",
    "            stack[-1] += f'{open_br}{mutated_inner}{close_br}'\n",
    "        else:\n",
    "            stack[-1] += ch\n",
    "\n",
    "    level_str = stack[0]\n",
    "\n",
    "    if swap_and_or and re.search(r'\\band\\b|\\bor\\b', level_str):\n",
    "        if random.random() < 0.5:\n",
    "            level_str = re.sub(r'\\band\\b', '__TMP_AND__', level_str)\n",
    "            level_str = re.sub(r'\\bor\\b', 'and', level_str)\n",
    "            level_str = re.sub(r'__TMP_AND__', 'or', level_str)\n",
    "\n",
    "    if swap_some_only and random.random() < 0.5:\n",
    "        matches = list(re.finditer(r'\\bsome\\b|\\bonly\\b', level_str))\n",
    "        if matches:\n",
    "            m = random.choice(matches)\n",
    "            orig = m.group(0)\n",
    "            repl = 'only' if orig == 'some' else 'some'\n",
    "            level_str = level_str[:m.start()] + repl + level_str[m.end():]\n",
    "\n",
    "    return level_str\n",
    "\n",
    "def mutate_axiom(expr: str,\n",
    "                 swap_some_only=True,\n",
    "                 swap_and_or=True) -> str:\n",
    "    # This function mutates an axiom expression by swapping \"some\" with \"only\" \n",
    "    # and/or swapping \"and\" with \"or\" based on the provided flags.\n",
    "    mutated = _mutate_axiom(expr, swap_some_only, swap_and_or)\n",
    "    if mutated == expr:\n",
    "        if re.search(r'\\band\\b|\\bor\\b', expr):\n",
    "            mutated = re.sub(r'\\band\\b', '__TMP_AND__', expr)\n",
    "            mutated = re.sub(r'\\bor\\b', 'and', mutated)\n",
    "            mutated = re.sub(r'__TMP_AND__', 'or', mutated)\n",
    "        elif re.search(r'\\bsome\\b|\\bonly\\b', expr):\n",
    "            matches = list(re.finditer(r'\\bsome\\b|\\bonly\\b', expr))\n",
    "            m = random.choice(matches)\n",
    "            orig = m.group(0)\n",
    "            repl = 'only' if orig == 'some' else 'some'\n",
    "            mutated = expr[:m.start()] + repl + expr[m.end():]\n",
    "    return mutated\n",
    "\n",
    "# test\n",
    "orig = \"[eats some (plants and animals)] and [drinks only water or milk]\"\n",
    "for _ in range(5):\n",
    "    print(mutate_axiom(orig))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca35d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# List of possible axiom predicates\n",
    "axiom_relations = [\n",
    "    \"subClassOf\", \"equivalentClass\", \"propertyRestrictions\",\n",
    "    \"disjointWith\", \"subPropertyOf\", \"domain\", \"range\",\n",
    "    \"characteristics\", \"inverseOf\"\n",
    "]\n",
    "\n",
    "def process_type3(input_path='type classification/type3.json',\n",
    "                  output_path='Final_type3.json'):\n",
    "    # 1. Load the original data\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 2. Traverse each ontology\n",
    "    for ontology in data.values():\n",
    "        # process both classes and properties\n",
    "        for section in ('classes', 'properties'):\n",
    "            for name, info in ontology.get(section, {}).items():\n",
    "                all_cq = info.get('CQ', [])\n",
    "                filtered = []\n",
    "                for entry in all_cq:\n",
    "                    ax = entry['axiom']\n",
    "                    # if the axiom is empty or contains \"None\" in domain/range, skip it\n",
    "                    if \" domain None\" in ax and \" range None\" in ax:\n",
    "                        continue\n",
    "                    filtered.append(entry)\n",
    "                cq_entries = filtered if filtered else all_cq\n",
    "                if not cq_entries:\n",
    "                    continue\n",
    "\n",
    "                # a) extract CQ entries with \"and\" or \"or\" or \"some\" or \"only\"\n",
    "                candidates = [\n",
    "                    entry for entry in cq_entries\n",
    "                    if re.search(r'\\b(and|or|some|only)\\b', entry['axiom'])\n",
    "                ]\n",
    "                if not candidates:\n",
    "                    # if and/or/some/only are not present in any axiom, skip this entity\n",
    "                    continue\n",
    "                sampled = random.choice(candidates)\n",
    "\n",
    "                # b) Parse the sampled axiom to find predicate and expression\n",
    "                axiom_str = sampled['axiom']\n",
    "                # mutate the axiom\n",
    "                info['editted axiom'] = mutate_axiom(axiom_str)\n",
    "                info['removed axiom'] = axiom_str\n",
    "\n",
    "                predicate = None\n",
    "                for rel in axiom_relations:\n",
    "                    if f\" {rel} \" in axiom_str:\n",
    "                        predicate = rel\n",
    "                        break\n",
    "                if predicate is None:\n",
    "                    # couldn't parse, skip\n",
    "                    continue\n",
    "\n",
    "                # split into subject and original expression\n",
    "                subject, expr = axiom_str.split(f\" {predicate} \", 1)\n",
    "                expr = expr.strip()\n",
    "\n",
    "                # split into edited expression\n",
    "                _, editted_expr = info['editted axiom'].split(f\" {predicate} \", 1)\n",
    "                editted_expr = editted_expr.strip()\n",
    "\n",
    "                # c) change the original expression to the edited expression \n",
    "                ax = info.get('axiom', {})\n",
    "                if predicate in ax and isinstance(ax[predicate], list):\n",
    "                    # if it's domain/range and expr is the literal \"None\", skip removal\n",
    "                    if not (predicate in ('domain', 'range') and expr == \"None\"):\n",
    "                        try:\n",
    "                            ax[predicate].remove(expr)\n",
    "                            ax[predicate].append(editted_expr)\n",
    "                        except ValueError:\n",
    "                            print(f\"Expression '{expr}' not found in axiom list for {name}.\")\n",
    "\n",
    "                # d) Build Target CQ and Valid CQ without deleting the original 'CQ'\n",
    "                target_cq = sampled['CQ']\n",
    "                valid_cq = [\n",
    "                    question\n",
    "                    for entry in all_cq\n",
    "                    if entry is not sampled\n",
    "                    for question in entry['CQ']\n",
    "                ]\n",
    "\n",
    "                # e) Attach new fields\n",
    "                info['Target CQ'] = target_cq\n",
    "                info['Valid CQ'] = valid_cq\n",
    "\n",
    "    # 3. Write out the processed file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "process_type3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f40c5b",
   "metadata": {},
   "source": [
    "Type 4 : Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cb4d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over2_axiom_num:  100\n"
     ]
    }
   ],
   "source": [
    "def process_type4(input_path='type classification/type4.json', output_path='Final_type4.json'):\n",
    "    # 1. Load the original data\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    over2_axiom_num =0\n",
    "    # 3. Process each ontology\n",
    "    for ontology in data.values():\n",
    "        for section in ('classes', 'properties'):\n",
    "            for name, info in ontology.get(section, {}).items():\n",
    "                temp_list = []\n",
    "                for entry in info.get('CQ', []):\n",
    "                    for cq in entry['CQ']:\n",
    "                        if cq not in temp_list:\n",
    "                            temp_list.append(cq)\n",
    "\n",
    "                info['Target CQ'] = random.sample(temp_list, min(3, len(temp_list)))\n",
    "                info['Valid CQ'] = temp_list\n",
    "                if len(temp_list)>3: over2_axiom_num+=1\n",
    "    print(\"over2_axiom_num: \", over2_axiom_num)\n",
    "                    \n",
    "\n",
    "    # 4. Write out the processed file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "process_type4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afce0c",
   "metadata": {},
   "source": [
    "Merge and transform to training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "total_dataset = {}\n",
    "train_dataset = {}\n",
    "test_dataset = {}\n",
    "# Load JSON files from the \"Final types\" directory\n",
    "final_types_directory = \"processed types\"\n",
    "for file_name in os.listdir(final_types_directory):\n",
    "    if file_name.endswith(\".json\"):  \n",
    "        with open(os.path.join(final_types_directory, file_name), \"r\", encoding=\"utf-8\") as json_file:\n",
    "            temp_dataset = json.load(json_file)\n",
    "            for ontology in temp_dataset:\n",
    "                if ontology not in total_dataset:\n",
    "                    total_dataset[ontology] = {}\n",
    "                for classorprop in temp_dataset[ontology]:\n",
    "                    if classorprop not in total_dataset[ontology]:\n",
    "                        total_dataset[ontology][classorprop] = {}\n",
    "                    for cp in temp_dataset[ontology][classorprop]:\n",
    "                        if cp not in total_dataset[ontology][classorprop]:\n",
    "                            total_dataset[ontology][classorprop][cp] = {}\n",
    "                        # Merge the data\n",
    "                        total_dataset[ontology][classorprop][cp].update(temp_dataset[ontology][classorprop][cp])\n",
    "                        # Split the data into train and test datasets (9:1 ratio) for each file name\n",
    "                    items = list(temp_dataset[ontology][classorprop].items())\n",
    "                    random.shuffle(items)\n",
    "                    split_index = int(len(items) * 0.9)\n",
    "                    train_items = dict(items[:split_index])\n",
    "                    test_items = dict(items[split_index:])\n",
    "\n",
    "                    if ontology not in train_dataset:\n",
    "                        train_dataset[ontology] = {}\n",
    "                    if ontology not in test_dataset:\n",
    "                        test_dataset[ontology] = {}\n",
    "\n",
    "                    if classorprop not in train_dataset[ontology]:\n",
    "                        train_dataset[ontology][classorprop] = {}\n",
    "                    if classorprop not in test_dataset[ontology]:\n",
    "                        test_dataset[ontology][classorprop] = {}\n",
    "\n",
    "                    for cp in train_items: train_dataset[ontology][classorprop][cp] = train_items[cp]\n",
    "                    for cp in test_items: test_dataset[ontology][classorprop][cp] = test_items[cp]\n",
    "# Save the merged data to a new JSON file\n",
    "with open(\"merged dataset/Final_dataset.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(total_dataset, json_file, indent=4, ensure_ascii=False)\n",
    "with open(\"merged dataset/Final_train_dataset.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(train_dataset, json_file, indent=4, ensure_ascii=False)\n",
    "with open(\"merged dataset/Final_test_dataset.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(test_dataset, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b733ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_construct(ontology, type, class_name,description, axiom, TCQ, VCQ, Taxiom, datatype, CQ):\n",
    "    return {\"data\":{\"input\": f\"\"\"As an ontology engineer, generate a list of competency questions based on the following description and axiom.\n",
    "Definition of competency questions: the questions that outline the scope of ontology and provide an idea about the knowledge that needs to be entailed in the ontology.\n",
    "Avoid using narrative questions + axioms.\n",
    "Don't generate unnecessary text. Output only the questions, separated by ` | ` (pipe with spaces). \n",
    "{type} name: {class_name}\n",
    "Description: {description}\n",
    "Axiom: {axiom}\n",
    "Generated CQs:\"\"\",\n",
    "            \"output\": f\"{TCQ[0]} | {TCQ[1]} | {TCQ[2]} \"},\n",
    "            \"metadata\": {\n",
    "            \"ontology\": ontology,\n",
    "            \"class\": class_name,\n",
    "            \"axiom\": axiom,\n",
    "            \"datatype\": datatype,\n",
    "            \"TCQ\": TCQ,\n",
    "            \"VCQ\": VCQ,\n",
    "            \"Taxiom\": Taxiom,\n",
    "            \"CQ\" : CQ\n",
    "                }}\n",
    "def save_dataset(dataset, output_path):\n",
    "    for ontology in dataset:\n",
    "        for classorprop in dataset[ontology]:\n",
    "            for cp in dataset[ontology][classorprop]:\n",
    "                if classorprop == \"classes\":\n",
    "                    type = \"Class\"\n",
    "                else:\n",
    "                    type = \"Property\"\n",
    "                axiom = dataset[ontology][classorprop][cp][\"axiom\"]\n",
    "                description = dataset[ontology][classorprop][cp][\"description\"]\n",
    "                TCQ = dataset[ontology][classorprop][cp][\"Target CQ\"]\n",
    "                VCQ = dataset[ontology][classorprop][cp][\"Valid CQ\"]\n",
    "                Taxiom = dataset[ontology][classorprop][cp][\"removed axiom\"] if \"removed axiom\" in dataset[ontology][classorprop][cp] else \"None\"\n",
    "                datatype = dataset[ontology][classorprop][cp][\"type\"]\n",
    "                CQ = dataset[ontology][classorprop][cp][\"CQ\"]\n",
    "                for cq in TCQ:\n",
    "                    if cq not in VCQ:\n",
    "                        VCQ.append(cq)\n",
    "                line_data = dataset_construct(ontology, type, cp,description,axiom, TCQ, VCQ, Taxiom, datatype, CQ)[\"data\"]\n",
    "                line_metadata = dataset_construct(ontology, type, cp,description,axiom, TCQ, VCQ, Taxiom, datatype, CQ)[\"metadata\"]\n",
    "                with open(f\"{output_path}.jsonl\", \"a\", encoding=\"utf-8\") as jsonl_file:\n",
    "                    jsonl_file.write(json.dumps(line_data, ensure_ascii=False) + \"\\n\")\n",
    "                with open(f\"{output_path}_meta.jsonl\", \"a\", encoding=\"utf-8\") as jsonl_file:\n",
    "                    jsonl_file.write(json.dumps(line_metadata, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(\"merged dataset/Final_train_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_dataset = json.load(f)\n",
    "with open(\"merged dataset/Final_test_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_dataset = json.load(f)    \n",
    "\n",
    "save_dataset(train_dataset, \"train_dataset\")\n",
    "save_dataset(test_dataset, \"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304ed444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "#Generalizablility setting(unseen ontology)\n",
    "onto_list = {\"AWO\": [\"AfricanWildlifeOntology1\"],\n",
    "\"OntoDT\": [\"OntoDT\"],\"SWO\": [\"swo\"],\"Pizza\": [\"pizza\"],\"Stuff\": [\"stuff\"],\n",
    "\"DEM@Care\": [\"lab\", \"time\", \"home\", \"exchangemodel\", \"event\"]}\n",
    "with open(\"merged dataset/Final_dataset.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    total_data = json.load(json_file)\n",
    "\n",
    "for onto in onto_list:\n",
    "    temp_train = copy.deepcopy(total_data)\n",
    "    temp_test = {}\n",
    "    for owl in onto_list[onto]:\n",
    "        temp_test[owl] = copy.deepcopy(total_data[owl])\n",
    "        del temp_train[owl]\n",
    "    save_dataset(temp_train, f\"additional settings/Generalizability/unseen ontology/{onto}/train_dataset\")\n",
    "    save_dataset(temp_test, f\"additional settings/Generalizability/unseen ontology/{onto}/test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d702463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyojun_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
